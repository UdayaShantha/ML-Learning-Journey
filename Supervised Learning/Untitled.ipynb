{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5e6d14f-90e3-48ca-a05a-1156be8ff5c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'speech_recognition'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspeech_recognition\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msr\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyttsx3\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'speech_recognition'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "\n",
    "class AIInterviewVoiceSystem:\n",
    "    def __init__(self):\n",
    "        # Text-to-Speech Engine (Free, Local)\n",
    "        self.tts_engine = pyttsx3.init()\n",
    "        \n",
    "        # Speech Recognition (Free, Local/Cloud)\n",
    "        self.recognizer = sr.Recognizer()\n",
    "        \n",
    "        # Advanced Speech Recognition Model (Free, Transformer-based)\n",
    "        self.speech_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        self.speech_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        \n",
    "        # Emotion Detection (Using pre-trained models)\n",
    "        self.emotion_model = self._load_emotion_model()\n",
    "    \n",
    "    def _load_emotion_model(self):\n",
    "        \"\"\"\n",
    "        Load a simple emotion detection model using transfer learning\n",
    "        This is a basic implementation that can be enhanced\n",
    "        \"\"\"\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(128,)),  # Feature vector input\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(5, activation='softmax')  # 5 emotion classes\n",
    "        ])\n",
    "        # Note: You'd typically load pre-trained weights here\n",
    "        return model\n",
    "    \n",
    "    def speak_question(self, question):\n",
    "        \"\"\"\n",
    "        Convert text question to speech\n",
    "        Uses pyttsx3 for local, free text-to-speech\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Configure voice properties\n",
    "            self.tts_engine.setProperty('rate', 150)  # Speaking rate\n",
    "            self.tts_engine.setProperty('volume', 0.9)  # Volume level\n",
    "            \n",
    "            # Speak the question\n",
    "            self.tts_engine.say(question)\n",
    "            self.tts_engine.runAndWait()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error in text-to-speech: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def record_answer(self, duration=10):\n",
    "        \"\"\"\n",
    "        Record candidate's answer\n",
    "        Uses sounddevice for audio recording\n",
    "        \"\"\"\n",
    "        print(\"Please speak your answer...\")\n",
    "        \n",
    "        # Record audio\n",
    "        fs = 44100  # Sample rate\n",
    "        recording = sd.rec(int(duration * fs), samplerate=fs, channels=1)\n",
    "        sd.wait()  # Wait until recording is finished\n",
    "        \n",
    "        # Save the recording\n",
    "        output_filename = \"candidate_answer.wav\"\n",
    "        sf.write(output_filename, recording, fs)\n",
    "        \n",
    "        return output_filename\n",
    "    \n",
    "    def transcribe_answer(self, audio_file):\n",
    "        \"\"\"\n",
    "        Transcribe recorded answer using multiple methods\n",
    "        \"\"\"\n",
    "        # Method 1: SpeechRecognition (supports multiple engines)\n",
    "        try:\n",
    "            with sr.AudioFile(audio_file) as source:\n",
    "                audio = self.recognizer.record(source)\n",
    "                \n",
    "                # Google Recognition (requires internet)\n",
    "                google_transcript = self.recognizer.recognize_google(audio)\n",
    "                \n",
    "                # Sphinx (offline recognition)\n",
    "                sphinx_transcript = self.recognizer.recognize_sphinx(audio)\n",
    "                \n",
    "                return {\n",
    "                    'google_transcript': google_transcript,\n",
    "                    'sphinx_transcript': sphinx_transcript\n",
    "                }\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Could not understand audio\")\n",
    "        except sr.RequestError:\n",
    "            print(\"Could not request results\")\n",
    "        \n",
    "        # Method 2: Wav2Vec2 (Transformer-based)\n",
    "        try:\n",
    "            # Load audio file\n",
    "            input_audio, sample_rate = librosa.load(audio_file, sr=16000)\n",
    "            \n",
    "            # Process audio\n",
    "            input_values = self.speech_processor(input_audio, sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n",
    "            \n",
    "            # Perform recognition\n",
    "            logits = self.speech_model(input_values).logits\n",
    "            predicted_ids = torch.argmax(logits, dim=-1)\n",
    "            transcription = self.speech_processor.batch_decode(predicted_ids)[0]\n",
    "            \n",
    "            return {'transformer_transcript': transcription}\n",
    "        except Exception as e:\n",
    "            print(f\"Transformer transcription error: {e}\")\n",
    "    \n",
    "    def detect_emotion(self, audio_file):\n",
    "        \"\"\"\n",
    "        Basic emotion detection from audio\n",
    "        Note: This is a placeholder and requires advanced feature extraction\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract audio features (placeholder)\n",
    "            features = self._extract_audio_features(audio_file)\n",
    "            \n",
    "            # Predict emotion\n",
    "            emotion_prediction = self.emotion_model.predict(features)\n",
    "            \n",
    "            # Map prediction to emotions\n",
    "            emotions = ['Neutral', 'Happy', 'Sad', 'Angry', 'Surprised']\n",
    "            detected_emotion = emotions[np.argmax(emotion_prediction)]\n",
    "            \n",
    "            return detected_emotion\n",
    "        except Exception as e:\n",
    "            print(f\"Emotion detection error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _extract_audio_features(self, audio_file):\n",
    "        \"\"\"\n",
    "        Extract features from audio file\n",
    "        This is a simplified placeholder\n",
    "        \"\"\"\n",
    "        # Use librosa for feature extraction\n",
    "        y, sr = librosa.load(audio_file)\n",
    "        \n",
    "        # Extract some basic features\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        \n",
    "        # Aggregate features\n",
    "        features = np.mean(mfccs.T, axis=0)\n",
    "        \n",
    "        return features.reshape(1, -1)  # Reshape for model input\n",
    "\n",
    "# Example Usage\n",
    "def interview_simulation():\n",
    "    # Initialize the interview voice system\n",
    "    interview_system = AIInterviewVoiceSystem()\n",
    "    \n",
    "    # Sample interview questions\n",
    "    questions = [\n",
    "        \"Tell me about your professional experience.\",\n",
    "        \"What motivates you to apply for this position?\",\n",
    "        \"Describe a challenging project you've worked on.\"\n",
    "    ]\n",
    "    \n",
    "    # Conduct interview simulation\n",
    "    for question in questions:\n",
    "        # Speak the question\n",
    "        interview_system.speak_question(question)\n",
    "        \n",
    "        # Record answer\n",
    "        answer_file = interview_system.record_answer(duration=10)\n",
    "        \n",
    "        # Transcribe answer\n",
    "        transcripts = interview_system.transcribe_answer(answer_file)\n",
    "        \n",
    "        # Detect emotion\n",
    "        emotion = interview_system.detect_emotion(answer_file)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\nQuestion:\", question)\n",
    "        print(\"Transcripts:\", transcripts)\n",
    "        print(\"Detected Emotion:\", emotion)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run the simulation\n",
    "if __name__ == \"__main__\":\n",
    "    interview_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89804df7-2860-460e-a9c4-800631447c33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b77560-42ed-4676-a1f6-62408fd887d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
